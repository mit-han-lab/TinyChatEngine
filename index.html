<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>TinyChatEngine: TinyChatEngine</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">TinyChatEngine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">TinyChatEngine </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> <img src="tinychat_logo.png" alt="" class="inline" title="tinychat_logo"/>    </p>
<h1><a class="anchor" id="autotoc_md0"></a>
TinyChatEngine: On-Device LLM Inference Library</h1>
<p>Running large language models (LLMs) on the edge is useful: copilot services (coding, office, smart reply) on laptops, cars, robots, and more. Users can get instant responses with better privacy, as the data is local.</p>
<p>This is enabled by LLM model compression technique: <a href="https://github.com/mit-han-lab/smoothquant">SmoothQuant</a> and <a href="https://github.com/mit-han-lab/llm-awq">AWQ (Activation-aware Weight Quantization)</a>, co-designed with TinyChatEngine that implements the compressed low-precision model.</p>
<p>Feel free to check out our <a href="assets/slides.pdf">slides</a> for more details!</p>
<h3><a class="anchor" id="autotoc_md1"></a>
Code LLaMA Demo on an NVIDIA GeForce RTX 4070 laptop:</h3>
<p><img src="coding_demo_gpu.gif" alt="" class="inline" title="coding_demo_gpu"/>    </p>
<h3><a class="anchor" id="autotoc_md2"></a>
LLaMA Chat Demo on an Apple MacBook Pro (M1, 2021):</h3>
<p><img src="chat_demo_m1.gif" alt="" class="inline" title="chat_demo_m1"/>    </p>
<h2><a class="anchor" id="autotoc_md3"></a>
Overview</h2>
<h3><a class="anchor" id="autotoc_md4"></a>
LLM Compression: SmoothQuant and AWQ</h3>
<p><a href="https://github.com/mit-han-lab/smoothquant">SmoothQuant</a>: Smooth the activation outliers by migrating the quantization difficulty from activations to weights, with a mathematically equal transformation (100*1 = 10*10).</p>
<div class="image">
<img src="smoothquant_intuition.png" alt=""/>
<div class="caption">
smoothquant_intuition</div></div>
    <p><a href="https://github.com/mit-han-lab/llm-awq">AWQ (Activation-aware Weight Quantization)</a>: Protect salient weight channels by analyzing activation magnitude as opposed to the weights.</p>
<h3><a class="anchor" id="autotoc_md5"></a>
LLM Inference Engine: TinyChatEngine</h3>
<ul>
<li><b>Universal</b>: x86 (Intel/AMD), ARM (Apple M1/M2, Raspberry Pi), CUDA (Nvidia GPU).</li>
<li><b>No library dependency</b>: From-scratch C/C++ implementation.</li>
<li><b>High performance</b>: Real-time on Macbook &amp; GeForce laptop.</li>
<li><b>Easy to use</b>: Download and compile, then ready to go!</li>
</ul>
<div class="image">
<img src="overview.png" alt=""/>
<div class="caption">
overview</div></div>
    <h2><a class="anchor" id="autotoc_md6"></a>
News</h2>
<ul>
<li>**(2023/10)** We extended the support for the coding assistant Code Llama. Feel free to check out.</li>
<li>**(2023/10)** âš¡We released the new CUDA backend to support Nvidia GPUs with compute capability &gt;= 6.1 for both server and edge GPUs. Its performance is also speeded up by ~40% compared to the previous version. Feel free to check out!</li>
<li>**(2023/09)** ðŸ”¥We released TinyVoiceChat, a voice chatbot that can be deployed on your edge devices, such as MacBook and Jetson Orin Nano. Check out our <a href="https://youtu.be/Bw5Dm3aWMnA?si=CCvZDmq3HwowEQcC">demo video</a> and step-by-step guide to deploy it on your device!</li>
</ul>
<h2><a class="anchor" id="autotoc_md7"></a>
Prerequisites</h2>
<h3><a class="anchor" id="autotoc_md8"></a>
MacOS</h3>
<p>For MacOS, install boost and llvm by</p>
<div class="fragment"><div class="line">brew install boost</div>
<div class="line">brew install llvm</div>
</div><!-- fragment --><p>For M1/M2 users, install Xcode from AppStore to enable the metal compiler for GPU support.</p>
<h3><a class="anchor" id="autotoc_md9"></a>
Windows with CPU</h3>
<p>For Windows, download and install the GCC compiler with MSYS2. Follow this tutorial: <a href="https://code.visualstudio.com/docs/cpp/config-mingw">https://code.visualstudio.com/docs/cpp/config-mingw</a> for installation.</p>
<ul>
<li>Install required dependencies with MSYS2</li>
</ul>
<div class="fragment"><div class="line">pacman -S --needed base-devel mingw-w64-x86_64-toolchain make unzip git</div>
</div><!-- fragment --><ul>
<li>Add binary directories (e.g., C:\msys64\mingw64\bin and C:\msys64\usr\bin) to the environment path</li>
</ul>
<h3><a class="anchor" id="autotoc_md10"></a>
Windows with Nvidia GPU (Experimental)</h3>
<ul>
<li>Install CUDA toolkit for Windows (<a href="https://developer.nvidia.com/cuda-toolkit">link</a>). When installing CUDA on your PC, please change the installation path to another one that does not include "spaces".</li>
<li>Install Visual Studio with C and C++ support: Follow the <a href="https://learn.microsoft.com/en-us/cpp/build/vscpp-step-0-installation?view=msvc-170">Instruction</a>.</li>
<li>Follow the instructions below and use x64 Native Tools Command Prompt from Visual Studio to compile TinyChatEngine.</li>
</ul>
<h2><a class="anchor" id="autotoc_md11"></a>
Step-by-step to Deploy LLaMA2-7B-chat with TinyChatEngine</h2>
<p>Here, we provide step-by-step instructions to deploy LLaMA2-7B-chat with TinyChatEngine from scratch.</p>
<ul>
<li>Download the repo. <div class="fragment"><div class="line">git clone --recursive https://github.com/mit-han-lab/TinyChatEngine</div>
<div class="line">cd TinyChatEngine</div>
</div><!-- fragment --></li>
<li>Install Python Packages<ul>
<li>The primary codebase of TinyChatEngine is written in pure C/C++. The Python packages are only used for downloading (and converting) models from our model zoo. <div class="fragment"><div class="line">conda create -n TinyChatEngine python=3.10 pip -y</div>
<div class="line">conda activate TinyChatEngine</div>
<div class="line">pip install -r requirements.txt</div>
</div><!-- fragment --></li>
</ul>
</li>
<li>Download the quantized LLaMA2-7B-chat model from our model zoo. <div class="fragment"><div class="line">cd llm</div>
</div><!-- fragment --><ul>
<li>On an x86 device (e.g., Intel/AMD laptop) <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On an ARM device (e.g., M1/M2 Macbook, Raspberry Pi) <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_ARM</div>
</div><!-- fragment --></li>
<li>On a CUDA device (e.g., Jetson AGX Orin, PC/Server) <code>bash python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_CUDA </code></li>
<li>Check this table for the detailed list of supported models</li>
</ul>
</li>
<li>*(CUDA only)* Based on the platform you are using and the compute capability of your GPU, modify the Makefile accordingly. If using Windows with Nvidia GPU, please modify <code>-arch=sm_xx</code> in <a href="llm/Makefile#L54">Line 54</a>. If using other platforms with Nvidia GPU, please modify <code>-gencode arch=compute_xx,code=sm_xx</code> in <a href="llm/Makefile#L60">Line 60</a>.</li>
<li>Compile and start the chat locally. <div class="fragment"><div class="line">make chat -j</div>
<div class="line">./chat</div>
<div class="line"> </div>
<div class="line">TinyChatEngine by MIT HAN Lab: https://github.com/mit-han-lab/TinyChatEngine</div>
<div class="line">Using model: LLaMA2_7B_chat</div>
<div class="line">Using AWQ for 4bit quantization: https://github.com/mit-han-lab/llm-awq</div>
<div class="line">Loading model... Finished!</div>
<div class="line">USER: Write a syllabus for Operating Systems.</div>
<div class="line">ASSISTANT:</div>
<div class="line">Of course! Here is a sample syllabus for a college-level course on operating systems:</div>
<div class="line">Course Title: Introduction to Operating Systems</div>
<div class="line">Course Description: This course provides an overview of the fundamental concepts and techniques used in modern operating systems, including process management, memory management, file systems, security, and I/O devices. Students will learn how these components work together to provide a platform for running applications and programs on a computer.</div>
<div class="line">Course Objectives:</div>
<div class="line">* Understand the basic architecture of an operating system</div>
<div class="line">* Learn about processes, threads, and process scheduling algorithms</div>
<div class="line">* Study memory management techniques such as paging and segmentation</div>
<div class="line">* Explore file systems including file organization, storage devices, and file access methods</div>
<div class="line">* Investigate security mechanisms to protect against malicious software attacks</div>
<div class="line">* Analyze input/output (I/O) operations and their handling by the operating system</div>
<div class="line">...</div>
</div><!-- fragment --></li>
</ul>
<h2><a class="anchor" id="autotoc_md12"></a>
Backend Support</h2>
<p>| Precision | x86<br  />
 (Intel/AMD CPU) | ARM<br  />
 (Apple M1/M2 &amp; RPi) | Nvidia GPU | Apple GPU | | ---&mdash; | ------------------------&mdash; | ------&mdash; | ------&mdash; | ------&mdash; | | FP32 | âœ… | âœ… | | | W4A16 | | | âœ… | âœ… | W4A32 | âœ… | âœ… | | âœ… | W4A8 | âœ… | âœ… | | | W8A8 | âœ… | âœ… | |</p>
<ul>
<li>For Raspberry Pi, we recommend using the board with 8GB RAM. Our testing was primarily conducted on Raspberry Pi 4 Model B Rev 1.4 with aarch64. For other versions, please feel free to try it out and let us know if you encounter any issues.</li>
<li>For Nvidia GPU, our CUDA backend can support Nvidia GPUs with compute capability &gt;= 6.1. For the GPUs with compute capability &lt; 6.1, please feel free to try it out but we haven't tested it yet and thus cannot guarantee the results.</li>
</ul>
<h2><a class="anchor" id="autotoc_md13"></a>
Quantization and Model Support</h2>
<p>The goal of TinyChatEngine is to support various quantization methods on various devices. For example, At present, it supports the quantized weights for int8 opt models that originate from <a href="https://github.com/mit-han-lab/smoothquant">smoothquant</a> using the provided conversion script <a href="llm/tools/opt_smooth_exporter.py">opt_smooth_exporter.py</a>. For LLaMA models, scripts are available for converting Huggingface format checkpoints to our int4 wegiht <a href="llm/tools/llama_exporter.py">format</a>, and for quantizing them to specific methods <a href="llm/tools/model_quantizer.py">based on your device</a>. Before converting and quantizing your models, it is recommended to apply the fake quantization from <a href="https://github.com/mit-han-lab/llm-awq">AWQ</a> to achieve better accuracy. We are currently working on supporting more models, please stay tuned!</p>
<h3><a class="anchor" id="autotoc_md14"></a>
Device-specific int4 Weight Reordering</h3>
<p>To mitigate the runtime overheads associated with weight reordering, TinyChatEngine conducts this process offline during model conversion. In this section, we will explore the weight layouts of QM_ARM and QM_x86. These layouts are tailored for ARM and x86 CPUs, supporting 128-bit SIMD and 256-bit SIMD operations, respectively. We also support QM_CUDA for Nvidia GPUs, including server and edge GPUs.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Platforms   </th><th class="markdownTableHeadNone">ISA   </th><th class="markdownTableHeadNone">Quantization methods    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Intel &amp; AMD   </td><td class="markdownTableBodyNone">x86-64   </td><td class="markdownTableBodyNone">QM_x86    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Apple M1/M2 Mac &amp; Raspberry Pi   </td><td class="markdownTableBodyNone">ARM   </td><td class="markdownTableBodyNone">QM_ARM    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Nvidia GPU   </td><td class="markdownTableBodyNone">CUDA   </td><td class="markdownTableBodyNone">QM_CUDA   </td></tr>
</table>
<ul>
<li>Example layout of QM_ARM: For QM_ARM, consider the initial configuration of a 128-bit weight vector, [w0, w1, ... , w30, w31], where each wi is a 4-bit quantized weight. TinyChatEngine rearranges these weights in the sequence [w0, w16, w1, w17, ..., w15, w31] by interleaving the lower half and upper half of the weights. This new arrangement facilitates the decoding of both the lower and upper halves using 128-bit AND and shift operations, as depicted in the subsequent figure. This will eliminate runtime reordering overheads and improve performance.</li>
</ul>
<h2><a class="anchor" id="autotoc_md15"></a>
Download and Deploy Models from our Model Zoo</h2>
<p>We offer a selection of models that have been tested with TinyChatEngine. These models can be readily downloaded and deployed on your device. To download a model, locate the target model's ID in the table below and use the associated script.</p>
<table class="doxtable">
<tr>
<th>Models </th><th>Precisions </th><th>ID </th><th>x86 backend </th><th>ARM backend </th><th>CUDA backend    </th></tr>
<tr>
<td rowspan="2">LLaMA2_13B_chat </td><td>fp32 </td><td>LLaMA2_13B_chat_fp32  </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>LLaMA2_13B_chat_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td>âœ…   </td></tr>
<tr>
<td rowspan="2">LLaMA2_7B_chat </td><td>fp32 </td><td>LLaMA2_7B_chat_fp32  </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>LLaMA2_7B_chat_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td>âœ…   </td></tr>
<tr>
<td rowspan="2">LLaMA_7B </td><td>fp32 </td><td>LLaMA_7B_fp32  </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>LLaMA_7B_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td>âœ…   </td></tr>
<tr>
<td rowspan="2">CodeLLaMA_13B_Instruct </td><td>fp32 </td><td>CodeLLaMA_13B_Instruct_fp32  </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>CodeLLaMA_13B_Instruct_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td>âœ…   </td></tr>
<tr>
<td rowspan="2">CodeLLaMA_7B_Instruct </td><td>fp32 </td><td>CodeLLaMA_7B_Instruct_fp32  </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>CodeLLaMA_7B_Instruct_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td>âœ…   </td></tr>
<tr>
<td rowspan="3">opt-6.7B </td><td>fp32 </td><td>opt_6.7B_fp32 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int8 </td><td>opt_6.7B_smooth_int8 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>opt_6.7B_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td rowspan="3">opt-1.3B </td><td>fp32 </td><td>opt_1.3B_fp32 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int8 </td><td>opt_1.3B_smooth_int8 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>opt_1.3B_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td rowspan="3">opt-125m </td><td>fp32 </td><td>opt_125m_fp32 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int8 </td><td>opt_125m_smooth_int8 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
<tr>
<td>int4 </td><td>opt_125m_awq_int4 </td><td>âœ…  </td><td>âœ…  </td><td></td></tr>
</table>
<p>For instance, to download the quantized LLaMA-2-7B-chat model: (for int4 models, use &ndash;QM to choose the quantized model for your device)</p>
<ul>
<li>On an Intel/AMD latptop: <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On an M1/M2 Macbook: <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_ARM</div>
</div><!-- fragment --></li>
<li>On an Nvidia GPU: <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_CUDA</div>
</div><!-- fragment --></li>
</ul>
<p>To deploy a quantized model with TinyChatEngine, compile and run the chat program.</p>
<ul>
<li>On CPU platforms <div class="fragment"><div class="line">make chat -j</div>
<div class="line"># ./chat &lt;model_name&gt; &lt;precision&gt; &lt;num_threads&gt;</div>
<div class="line">./chat LLaMA2_7B_chat INT4 8</div>
</div><!-- fragment --></li>
<li>On GPU platforms <div class="fragment"><div class="line">make chat -j</div>
<div class="line"># ./chat &lt;model_name&gt; &lt;precision&gt;</div>
<div class="line">./chat LLaMA2_7B_chat INT4</div>
</div><!-- fragment --></li>
</ul>
<h2><a class="anchor" id="autotoc_md16"></a>
Experimental Features</h2>
<h3><a class="anchor" id="autotoc_md17"></a>
Voice Chatbot <a href="https://youtu.be/Bw5Dm3aWMnA?si=CCvZDmq3HwowEQcC">[Demo]</a></h3>
<p>TinyChatEngine offers versatile capabilities suitable for various applications. Additionally, we introduce a sophisticated voice chatbot. Explore our step-by-step guide here to seamlessly deploy a speech-to-speech chatbot locally on your device!</p>
<h2><a class="anchor" id="autotoc_md18"></a>
Related Projects</h2>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine: Memory-efficient and High-performance Neural Network Library for Microcontrollers</a></p>
<p><a href="https://github.com/mit-han-lab/smoothquant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></p>
<p><a href="https://github.com/mit-han-lab/llm-awq">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></p>
<h2><a class="anchor" id="autotoc_md19"></a>
Acknowledgement</h2>
<p><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></p>
<p><a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a></p>
<p><a href="https://github.com/huggingface/transformers">transformers</a> </p>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"/>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0
</small></address>
</body>
</html>
